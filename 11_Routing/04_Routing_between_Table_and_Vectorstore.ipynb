{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38d7c3f7",
   "metadata": {},
   "source": [
    "# üö¶ Routing tra SQL, Chat e Off Topic\n",
    "\n",
    "## üéØ Obiettivo\n",
    "\n",
    "Creare un **sistema di routing** che analizzi la domanda dell‚Äôutente e scelga dinamicamente il tipo di risposta tra:\n",
    "\n",
    "| Categoria   | Azione                                                     |\n",
    "| ----------- | ---------------------------------------------------------- |\n",
    "| `database`  | Eseguire query SQL sulla tabella `products`                |\n",
    "| `chat`      | Eseguire catena RAG classica (retriever + LLM)             |\n",
    "| `off_topic` | Rispondere con un messaggio statico (nessuna chiamata LLM) |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Prompt di classificazione\n",
    "\n",
    "Creiamo un **modello di classificazione** istruito per assegnare una delle tre etichette a ogni domanda:\n",
    "\n",
    "```txt\n",
    "Classifica la seguente domanda in una delle tre categorie:\n",
    "\n",
    "1. database ‚Äî per domande su prodotti o ordini\n",
    "2. chat ‚Äî per domande generiche sul ristorante (es. orari)\n",
    "3. off_topic ‚Äî per qualsiasi altra cosa (meteo, calcio, ecc.)\n",
    "\n",
    "Domanda: \"{{question}}\"\n",
    "Categoria: \n",
    "```\n",
    "\n",
    "### ‚úèÔ∏è Esempi:\n",
    "\n",
    "* ‚ÄúQual √® il dessert pi√π costoso?‚Äù ‚Üí `database` (informazione contenuta nel DB)\n",
    "* ‚ÄúChi √® il proprietario del ristorante?‚Äù ‚Üí `chat` (informazione contenuta nel Vectostore RAG)\n",
    "* ‚ÄúCom'√® il tempo oggi?‚Äù ‚Üí `off_topic`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20d7fac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "classification_prompt = \"\"\"You are good at classify a question.\n",
    "Given the user question below, classify it as either being about 'Database', 'Chat', or 'Offtopic'.\n",
    "\n",
    "<If the question is about products of the restaurant OR ordering food classify the question as 'Database'>\n",
    "<If the question is about restaurant related topics like opening hours and similar topics, classify it as 'Chat'>\n",
    "<If the question is about whether, football or anything not related to the restaurant or products, classify the question as 'Offtopic'>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Classification:\n",
    "\"\"\"\n",
    "\n",
    "classification_template = PromptTemplate.from_template(classification_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "464725e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "classification_chain = classification_template | ChatOpenAI() | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92677c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Offtopic'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_chain.invoke({\"question\": \"How is the weather?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2a88eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üõ†Ô∏è Step 2 ‚Äì Router Logico\n",
    "\n",
    "Usiamo un `RunnableLambda` per decidere il flusso da seguire:\n",
    "\n",
    "```python\n",
    "def route_by_topic(data):\n",
    "    topic = data[\"topic\"]\n",
    "    if topic == \"database\":\n",
    "        return sql_chain\n",
    "    elif topic == \"chat\":\n",
    "        return rag_chain\n",
    "    else:\n",
    "        return StaticResponse(\"Mi dispiace, non posso rispondere su questo argomento.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c1b84a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\felip\\Desktop\\Advanced_RAG\\.venv\\Lib\\site-packages\\langchain_community\\utilities\\sql_database.py:134: SAWarning: Did not recognize type 'vector' of column 'embedding'\n",
      "  self._metadata.reflect(\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "\n",
    "template = \"\"\"Based on the table schema below, write a SQL query that would answer the user's question:\n",
    "{schema}\n",
    "\n",
    "Question: {question}\n",
    "SQL Query:\"\"\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "CONNECTION_STRING = (\n",
    "    \"postgresql+psycopg2://admin:admin@localhost:5432/vectordb\"\n",
    ")\n",
    "\n",
    "# qui importiamo il database SQL \n",
    "# otteniamo l'istanza del nostro databse\n",
    "db = SQLDatabase.from_uri(CONNECTION_STRING)\n",
    "\n",
    "# definiamo la funzione per ottenere lo schema \n",
    "# fornisce le informazioni al nostro modello\n",
    "# tutte le informazioni sul nostro database e sulle tabelle saranno fornite \n",
    "# da questa funzione \n",
    "def get_schema(_):\n",
    "    schema = db.get_table_info()\n",
    "    return schema\n",
    "\n",
    "# funzione che esegue sul database la query \n",
    "# generata dal modello \n",
    "def run_query(query):\n",
    "    return db.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65defde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, inspect\n",
    "from tabulate import tabulate\n",
    "\n",
    "# ci√≤ di cui abbiamo sono solo le informazioni sulla nostra tabella products\n",
    "def get_schema(_):\n",
    "    engine = create_engine(CONNECTION_STRING)\n",
    "\n",
    "    inspector = inspect(engine)\n",
    "\n",
    "    columns = inspector.get_columns(\"products\")\n",
    "\n",
    "    column_data = [\n",
    "        {\n",
    "            \"Column Name\": col[\"name\"],\n",
    "            \"Data Type\": str(col[\"type\"]),\n",
    "            \"Nullable\": \"Yes\" if col[\"nullable\"] else \"No\",\n",
    "            \"Default\": col[\"default\"] if col[\"default\"] else \"None\",\n",
    "            \"Autoincrement\": \"Yes\" if col[\"autoincrement\"] else \"No\"\n",
    "        }\n",
    "        for col in columns\n",
    "    ]\n",
    "\n",
    "\n",
    "    schema_output = tabulate(column_data, headers=\"keys\", tablefmt=\"grid\")\n",
    "\n",
    "    formatted_schema = f\"Schema for 'products' table:\\n{schema_output}\"\n",
    "\n",
    "    return formatted_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30066caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizziamo tale schema per creare la nostra chain\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# utilizziamo un metodo definito (.bind()) che lega gli argomenti del runtime ai Runnable\n",
    "# qui leghiamo l'argomento stop al modello\n",
    "# quindi vogliamo impedire all'LLM di generare token dopo aver creato\n",
    "# la nostra query SQL, dato che vogliamo solo la query SQL e nient'altro\n",
    "\n",
    "sql_response = (\n",
    "    RunnablePassthrough.assign(schema=get_schema)\n",
    "    | prompt\n",
    "    | model.bind(stop=[\"\\nSQLResult:\"]) \n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a765279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "template = \"\"\"Based on the table schema below, question, sql query, and sql response, write a natural language response:\n",
    "{schema}\n",
    "\n",
    "Question: {question}\n",
    "SQL Query: {query}\n",
    "SQL Response: {response}\"\"\"\n",
    "\n",
    "prompt_response = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def debug(input):\n",
    "    print(\"SQL Output: \", input[\"query\"])\n",
    "    return input\n",
    "\n",
    "sql_chain = (\n",
    "    RunnablePassthrough.assign(query=sql_response).assign(\n",
    "        schema=get_schema,\n",
    "        response=lambda x: run_query(x['query'])mbedding\n",
    "    )\n",
    "    | RunnableLambda(debug)\n",
    "    | prompt_response\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "500eac50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felip\\AppData\\Local\\Temp\\ipykernel_28728\\1967423982.py:12: LangChainPendingDeprecationWarning: Please use JSONB instead of JSON for metadata. This change will allow for more efficient querying that involves filtering based on metadata. Please note that filtering operators have been changed when using JSONB metadata to be prefixed with a $ sign to avoid name collisions with columns. If you're using an existing database, you will need to create a db migration for your metadata column to be JSONB and update your queries to use the new operators. \n",
      "  store = PGVector(\n"
     ]
    }
   ],
   "source": [
    "# RAG Chain\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.text import TextLoader\n",
    "from langchain_community.vectorstores.pgvector import PGVector\n",
    "\n",
    "DATABASE_URL = \"postgresql+psycopg2://admin:admin@localhost:5432/vectordb\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "store = PGVector(\n",
    "    collection_name=\"vectordb\",\n",
    "    connection_string=DATABASE_URL,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "loader1 = TextLoader(\"./data/restaurant.txt\")\n",
    "loader2 = TextLoader(\"./data/founder.txt\")\n",
    "\n",
    "docs1 = loader1.load()\n",
    "docs2 = loader2.load()\n",
    "\n",
    "docs = docs1 + docs2\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=50)\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "store.add_documents(chunks)\n",
    "\n",
    "retriever = store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04458444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \n",
    "     \"question\": itemgetter(\"question\")}\n",
    "    | rag_prompt\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "938ca864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chef Amico is the owner of the restaurant.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke({\"question\": \"Who is the owner of the restaurant?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df04cabe",
   "metadata": {},
   "source": [
    "## Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc9c9e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route(info):\n",
    "    if \"database\" in info['topic'].lower():\n",
    "        print(\"Using sql_chain\")\n",
    "        return sql_chain\n",
    "    elif \"chat\" in info[\"topic\"].lower():\n",
    "        print(\"Using chain\")\n",
    "        return rag_chain\n",
    "    else: # offtopic\n",
    "        return \"I am sorry, I am not allowed to answer about this topic.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6927d7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "\n",
    "full_chain = RunnableParallel(\n",
    "    {\n",
    "        \"topic\": classification_chain,\n",
    "        \"question\": lambda x: x['question']\n",
    "    }\n",
    ") | RunnableLambda(route)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5715c9ae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÅ Catena Finale Composta\n",
    "\n",
    "```\n",
    "User Input\n",
    "   ‚Üì\n",
    "Parallel (pass-through) ‚Üí Classificatore (‚Üí \"topic\")\n",
    "   ‚Üì\n",
    "Router (RunnableLambda con logica `if topic == ...`)\n",
    "   ‚Üí SQL chain\n",
    "   ‚Üí RAG chain\n",
    "   ‚Üí Static message\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Esempi di Test\n",
    "\n",
    "### ‚ùì Domanda: ‚ÄúQual √® il dessert pi√π costoso che offrite?‚Äù\n",
    "\n",
    "* üîÄ Routing ‚Üí `database`\n",
    "* ‚úÖ Output: ‚ÄúIl panettone, al prezzo di 15 dollari.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùì Domanda: ‚ÄúCom‚Äô√® il tempo domani?‚Äù\n",
    "\n",
    "* üîÄ Routing ‚Üí `off_topic`\n",
    "* ‚úÖ Output: ‚ÄúMi dispiace, non posso rispondere su questo argomento.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùì Domanda: ‚ÄúChi √® il proprietario del ristorante?‚Äù\n",
    "\n",
    "* üîÄ Routing ‚Üí `chat`\n",
    "* ‚úÖ Output: ‚ÄúChef Amico.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c700597d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sql_chain\n",
      "SQL Output:  SELECT name, price\n",
      "FROM products\n",
      "WHERE category = 'dessert'\n",
      "ORDER BY price DESC\n",
      "LIMIT 1;\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The most expensive dessert we offer is the panettone priced at $15.00.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"Whats the most expensive dessert you offer?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58dab636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am sorry, I am not allowed to answer about this topic.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"How will the weather be tomorrow?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d70cc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using chain\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Chef Amico.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"Who is the owner of the restaurant?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b376d792",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîê Note sulla Sicurezza\n",
    "\n",
    "* Il routing evita **chiamate non necessarie** all‚ÄôLLM (es. su argomenti off topic).\n",
    "* Protegge la **coerenza del dominio** (non vogliamo che un bot aziendale risponda sul meteo o sulla politica).\n",
    "* √à un primo passo verso un **controllo conversazionale pi√π robusto**.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Conclusione\n",
    "\n",
    "Hai imparato a:\n",
    "\n",
    "* Integrare query SQL con LLM in modo sicuro (accesso in sola lettura).\n",
    "* Costruire un classificatore LLM-based per instradare le domande.\n",
    "* Gestire con eleganza i casi fuori dominio (`off_topic`).\n",
    "\n",
    "> üîú Nella prossima lezione impareremo a usare **[Nemo Guardrails](https://nemoguardrails.ai/)** per rafforzare ancora di pi√π la **sicurezza e il controllo dei flussi conversazionali**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
