{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d103a2e6",
   "metadata": {},
   "source": [
    "# üß≠ **Routing: Come Scegliere il Miglior Esperto (o Tool) in Base alla Domanda**\n",
    "\n",
    "---\n",
    "\n",
    "## üîç **Cos'√® il Routing? Perch√© √® Importante?**\n",
    "\n",
    "Gli LLM sono potenti‚Ä¶ ma non onniscienti!\n",
    "\n",
    "* üß† Gli LLM sono **modelli linguistici** addestrati a **predire token**, non a interpretare **strutture tabellari** o decidere **quale strumento usare** per un dato input.\n",
    "* üßÆ Se fornisci una tabella a un LLM, essa verr√† trattata come una **singola lunga stringa**, causando perdita di struttura.\n",
    "\n",
    "![alt](../images/routing_1.png)\n",
    "\n",
    "* üõ†Ô∏è **Approcci alternativi**:\n",
    "\n",
    "  * Query SQL su database strutturati üìä: Memorizzare i dati in un database SQL e lasciare che un LLM scriva query per interrogare il database\n",
    "  * Tool calling per task specializzati üß∞: Far si che un LLM interagisca con funzioni personalizzate \n",
    "\n",
    "  * Possiamo utilizzare entrambi nella stssa applicazione ma per farlo dobbiamo utilizzare il **Routing**\n",
    "\n",
    "üß† **Routing** √® il meccanismo per decidere, in automatico, **quale approccio utilizzare (interrogare un DB o usare un Tool)** usare per rispondere a una domanda in base alla sua natura e quindi inidirizzarci verso l'una o l'altra implementazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dcb9767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698ef675",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèóÔ∏è Esempio: Tre Esperti LLM\n",
    "\n",
    "Hai 3 LLM specializzati:\n",
    "\n",
    "1. üöó **Automobili** ‚Äì Domande su auto, motori, batterie\n",
    "2. üçΩÔ∏è **Cibo e ristoranti** ‚Äì Critico gastronomico\n",
    "3. üíª **Tecnologia** ‚Äì Software, gadget, AI\n",
    "\n",
    "Obiettivo: capire a **quale esperto instradare la domanda**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99db3fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_template = \"\"\"You are an expert in automobiles. You have extensive knowledge about car mechanics, models, and automotive technology.\n",
    "You provide clear and helpful answers about cars.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "restaurant_template = \"\"\"You are a knowledgeable foo critic and restaurant reviewer. You have a deep understanding of different cuisines, dining experiences, and what makes a great restaurant.\n",
    "You provide clear and helpful answers about cars.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "technology_template = \"\"\"You are a tech expert with in-depth knowledge of the latest gadgets, software, and technological trends.\n",
    "You provide insightful and detailed answers about technology.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6877390d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Quando facciamo una domanda sul cibo, vogliamo chiedere all'esperto di cibo.\n",
    "\n",
    "Dunque, in qualche modo, dobbiamo indirizzare la domanda all'LLM esperto in cibo.\n",
    "\n",
    "Analogamente vale lo stesso ragionamento con gli altri due esperti LLM.\n",
    "\n",
    "Un approccio consiste nell'utilizzare domande esemplificative come quelle sulle auto, sui ristoranti e sulla tecnologia.\n",
    "\n",
    "Dopodich√®, utilizzare un modello di embedding per creare gli emebddings di queste domande.\n",
    "\n",
    "Dopo che abbiamo ottenuto gli embeddings, 3 per ogni categoria, possiamo utilizzarli nella nostra funzione esterna per calcolare la cosine similarity tra una query e tali embeddings. Prendiamo il massimo score di similarity delle tre categorie e di questi tre scores il massimo score. Sapremo quindi con quale embedding la query ha la maggiore similarity e quindi anche la categoria di tale embedding. Possiamo quindi instradare la query verso l'LLM esperto in tale categoria. \n",
    "\n",
    "\n",
    "## üî¢ üîÄ Approccio 1 ‚Äì Routing via **Embedding Similarity**\n",
    "\n",
    "### üõ†Ô∏è Step 1: Crea esempi per ogni categoria\n",
    "\n",
    "```text\n",
    "Domande di esempio ‚Üí embeddings ‚Üí 3 vettori per categoria (auto, food, tech)\n",
    "```\n",
    "\n",
    "### üßÆ Step 2: Similarit√† Coseno\n",
    "\n",
    "Confronta la **query in arrivo** con gli esempi tramite **cosine similarity** (`langchain.utils.math.cosine_similarity`):\n",
    "\n",
    "```python\n",
    "similarity = cosine_similarity(query_embedding, example_embedding)\n",
    "```\n",
    "\n",
    "Prendi il valore massimo per ciascuna categoria e instrada verso:\n",
    "\n",
    "* `car_template` se max √® in auto\n",
    "* `restaurant_template` se max √® in food\n",
    "* `tech_template` se max √® in tech\n",
    "\n",
    "### ‚úÖ Esempio\n",
    "\n",
    "**Query:** ‚ÄúWhat‚Äôs the best way to improve my car‚Äôs battery life?‚Äù\n",
    "üîÅ Viene instradata a ‚Üí `car_template` ‚úîÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71d4eed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_questions = [\n",
    "    \"What is the difference between a sedan and a SUV?\",\n",
    "    \"How does a hybrid car save fuel?\",\n",
    "    \"What should I look for when buying a used car?\"\n",
    "]\n",
    "\n",
    "restaurant_questions = [\n",
    "    \"What makes a five-star restaurant exceptional?\",\n",
    "    \"How do I choose a good wine pairing for my meal?\",\n",
    "    \"What are the key elements of French cuisine?\"\n",
    "]\n",
    "\n",
    "technology_questions = [\n",
    "    \"What are the latest advancements in AI?\",\n",
    "    \"How do I secure my home network against cyber threats?\",\n",
    "    \"What should I consider when buying a new smartphone?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84d411d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "car_question_embeddings = embeddings.embed_documents(car_questions)\n",
    "\n",
    "restaurant_question_embeddings = embeddings.embed_documents(restaurant_questions)\n",
    "\n",
    "technology_question_embeddings = embeddings.embed_documents(technology_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f3e175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CAR\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.utils.math import cosine_similarity\n",
    "\n",
    "def prompt_router(input):\n",
    "    query_embedding = embeddings.embed_query(input['query'])\n",
    "    car_similarity = cosine_similarity([query_embedding], car_question_embeddings)[0]\n",
    "    restaurant_similarity = cosine_similarity([query_embedding], restaurant_question_embeddings)[0]\n",
    "    tech_similarity = cosine_similarity([query_embedding], technology_question_embeddings)[0]\n",
    "\n",
    "    max_similarity = max(max(car_similarity), max(restaurant_similarity), max(tech_similarity))\n",
    "\n",
    "    if max_similarity == max(car_similarity):\n",
    "        print(\"Using CAR\")\n",
    "        return PromptTemplate.from_template(car_template)\n",
    "    \n",
    "    elif max_similarity == max(restaurant_similarity):\n",
    "        print(\"Using RESTAURANT\")\n",
    "        return PromptTemplate.from_template(restaurant_template)\n",
    "    \n",
    "    else:\n",
    "        print(\"Using TECH\")\n",
    "        return PromptTemplate.from_template(restaurant_template)\n",
    "    \n",
    "input_query = {\"query\": \"What's the best way to improve my cars's battery life?\"}\n",
    "\n",
    "prompt = prompt_router(input_query)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eff0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integriamo tale funzione in una chain\n",
    "\n",
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router) # viene fatto .invoke() del prompt sulla query\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f82ecdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RESTAURANT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Identifying a good vintage wine at a restaurant can be a daunting task, but there are a few tips to keep in mind:\\n\\n1. Look for a reputable wine list: A good restaurant will typically have a well-curated wine list with a variety of options from different regions and vintages. Check for a diverse selection of wines and ask the sommelier for recommendations.\\n\\n2. Check the label: Look for wines from well-known and respected producers, as they are more likely to produce high-quality vintage wines. Also, check the year on the label to ensure that it is a good vintage.\\n\\n3. Ask about the wine's provenance: Inquire about how the wine has been stored and cared for, as proper storage conditions are crucial for preserving the quality of a vintage wine.\\n\\n4. Trust your palate: Ultimately, the best way to identify a good vintage wine is to trust your own taste buds. Don't be afraid to ask for a small taste before committing to a full glass or bottle.\\n\\nBy keeping these tips in mind, you can increase your chances of selecting a good vintage wine at a restaurant.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"How do I identify a good vintage wine at a restaurant?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b093cb",
   "metadata": {},
   "source": [
    "Ok, per il nostro esempio ha funzionato bene.\n",
    "\n",
    "Tuttavia ci possono essere molti casi limite, gli embeddings potrebbero essere non sufficienti.\n",
    "\n",
    "**Un'altro modo √® quello di far classificare direttamente la domanda a un LLM.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23623b1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß†üîÄ Approccio 2 ‚Äì Routing via **LLM Classifier**\n",
    "\n",
    "### üß† Prompt di Classificazione\n",
    "\n",
    "Prompt semplice:\n",
    "\n",
    "```\n",
    "You are good at classifying questions.\n",
    "Given a question, classify it as one of: car, restaurant, technology.\n",
    "```\n",
    "\n",
    "‚û°Ô∏è LLM risponde con `car`, `restaurant` o `technology`.\n",
    "\n",
    "### üß† Vantaggi\n",
    "\n",
    "* üß† **Miglior gestione dei casi limite**\n",
    "* üîß **Pi√π facile da mantenere**\n",
    "* üí∏ Usa modelli economici (es: `gpt-3.5-turbo`)\n",
    "* ‚ö° Veloce, stabile, accurato\n",
    "\n",
    "### ‚úÖ Esempio\n",
    "\n",
    "**Query:** ‚ÄúWhat are the latest trends in electric cars?‚Äù\n",
    "üîÅ Classificata come `technology` (e non `car`!), quindi ‚Üí `tech_template` ‚úîÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92fcd752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text=\"You are good at classify a question.\\nGiven the user question below, classify it as either being about 'Car', 'Restaurant', or 'Technology'.\\n\\n<If the question is about car mechanics, models, or automotive technology, classify it as 'Car'>\\n<If the question is about cuisines, dining experiences, or restaurant services, classify it as 'Restaurant'>\\n<If the question is about gadgets, software, or technological trends, classify it as 'Technology'>\\n\\n<question>\\ncio\\n</question>\\n\\nClassification:\")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creiamo un template di classificazione per il modello LLM\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "classification_prompt = \"\"\"You are good at classify a question.\n",
    "Given the user question below, classify it as either being about 'Car', 'Restaurant', or 'Technology'.\n",
    "\n",
    "<If the question is about car mechanics, models, or automotive technology, classify it as 'Car'>\n",
    "<If the question is about cuisines, dining experiences, or restaurant services, classify it as 'Restaurant'>\n",
    "<If the question is about gadgets, software, or technological trends, classify it as 'Technology'>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Classification:\"\"\"\n",
    "\n",
    "classification_template = PromptTemplate.from_template(classification_prompt)\n",
    "\n",
    "classification_template.invoke({\"question\": \"cio\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1530176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain \n",
    "classification_chain = classification_template | ChatOpenAI() | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74787938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Technology'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_query = {\"question\": \"What are the latest trends in electric cars?\"}\n",
    "\n",
    "classification_chain.invoke(input_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "684b1969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TECHNOLOGY\n"
     ]
    }
   ],
   "source": [
    "# utilizziamo la classification chain nel prompt router\n",
    "def prompt_router(input):\n",
    "    classification = classification_chain.invoke({'question': input['query']})\n",
    "\n",
    "    if classification == 'Car':\n",
    "        print(\"Using CAR\")\n",
    "        return PromptTemplate.from_template(car_template)\n",
    "    elif classification == \"Restaurant\":\n",
    "        print(\"Using RESTAURANT\")\n",
    "        return PromptTemplate.from_template(restaurant_template)\n",
    "    elif classification == \"Technology\":\n",
    "        print(\"Using TECHNOLOGY\")\n",
    "        return PromptTemplate.from_template(technology_template)\n",
    "\n",
    "    else:\n",
    "        print(\"Unexpected classification:\", classification)\n",
    "        return None\n",
    "\n",
    " \n",
    "input_query = {\"query\": \"What are the latest trends in electric cars?\"}\n",
    "\n",
    "prompt = prompt_router(input_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b759f97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integriamo tale router in una chain\n",
    "\n",
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4bd2a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RESTAURANT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"When identifying a good vintage wine at a restaurant, there are several factors to consider:\\n\\n1. Reputation of the winery: Research the winery and its reputation for producing high-quality wines. Look for well-known and reputable wineries that have a history of producing exceptional vintages.\\n\\n2. Vintage year: Pay attention to the vintage year of the wine, as this can greatly affect its quality. In general, wines from good vintage years tend to be more valuable and higher in quality.\\n\\n3. Label information: Look for detailed information on the label, such as the grape variety, region, and any special designations or classifications. Wines that are labeled with specific appellations or vineyard names are often indicative of higher quality.\\n\\n4. Price point: While price is not always indicative of quality, vintage wines tend to be more expensive due to their limited availability and aging potential. Be cautious of wines that are priced significantly lower than others of the same vintage and label.\\n\\n5. Seek recommendations: Ask the restaurant staff for recommendations on vintage wines, as they may have personal favorites or knowledge of which wines are currently showing well. Additionally, consider consulting a sommelier for guidance on selecting a vintage wine that suits your preferences.\\n\\nOverall, trust your palate and instincts when selecting a vintage wine at a restaurant, and don't be afraid to explore new varieties and regions to expand your wine knowledge and appreciation.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"How do I identify a good vintage wine at a restaurant?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a69a0b2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Confronto dei Due Metodi\n",
    "\n",
    "| ‚öôÔ∏è Metodo           | ‚úÖ Vantaggi                                  | ‚ö†Ô∏è Svantaggi               |\n",
    "| ------------------- | ------------------------------------------- | -------------------------- |\n",
    "| **Embedding-based** | ‚ú® Veloce con pochi esempi                   | üòì Debole con edge cases   |\n",
    "| **LLM Classifier**  | üîç Preciso, flessibile, facile da estendere | üí∏ Richiede chiamata a LLM |\n",
    "\n",
    "üëâ **Consiglio pratico**: **Usa LLM classifier** ‚Äì √® la scelta migliore per routing generalizzato e stabile.\n",
    "\n",
    "---\n",
    "\n",
    "## üîú **Prossimi Step**\n",
    "\n",
    "Nel prossimo modulo:\n",
    "\n",
    "‚û°Ô∏è Userai il **Routing** per decidere tra:\n",
    "\n",
    "* üìÑ Retrieval classico (RAG)\n",
    "* üíæ SQL agent che scrive query per te\n",
    "\n",
    "Preparati a costruire **applicazioni intelligenti multi-tool**! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
