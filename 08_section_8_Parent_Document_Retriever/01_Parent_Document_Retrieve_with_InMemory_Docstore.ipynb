{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22e1bd40",
   "metadata": {},
   "source": [
    "# üß± Parent-Child Retrieval e Persistenza con Postgres in LangChain\n",
    "\n",
    "## üß≠ Obiettivo\n",
    "\n",
    "Bilanciare **specificit√†** e **contesto** nella fase di retrieval dividendo i documenti in:\n",
    "\n",
    "* **Chunk piccoli (child)** ‚Üí embedding precisi per il recupero\n",
    "* **Chunk grandi (parent)** ‚Üí contesto utile nella risposta\n",
    "\n",
    "---\n",
    "\n",
    "## üìê Problema iniziale\n",
    "\n",
    "* Chunk troppo **piccoli** = embedding accurati, ma poco contesto\n",
    "* Chunk troppo **grandi** = contesto completo, ma embedding vaghi\n",
    "\n",
    "> ‚ùó Serve un modo per **recuperare documenti granulari**, ma **rispondere con documenti pi√π grandi**\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Soluzione: Parent-Child Retriever\n",
    "\n",
    "LangChain fornisce una struttura in cui:\n",
    "\n",
    "* I **chunk piccoli** vengono inseriti nel **vector store**\n",
    "* Ogni chunk mantiene un riferimento (`doc_id`) al suo **documento padre** nel **docstore**\n",
    "* In fase di retrieval:\n",
    "\n",
    "  1. Si cercano i chunk pi√π simili\n",
    "  2. Si leggono i `doc_id` associati\n",
    "  3. Si caricano i documenti completi (parent) dal `docstore`\n",
    "\n",
    "\n",
    "![alt](../images/parent-child.png)\n",
    "\n",
    "---\n",
    "\n",
    "## üõ† Composizione del sistema\n",
    "\n",
    "```txt\n",
    "          +----------------+         +-------------------+\n",
    "          |   VectorStore  |         |     DocStore      |\n",
    "          | (chunk piccoli)|         | (documenti grandi)|\n",
    "          +----------------+         +-------------------+\n",
    "                  |                           |\n",
    "      [retrieval con embedding]      [lookup da doc_id]\n",
    "                  |                           |\n",
    "            +-------------------------------+\n",
    "            |   Parent Documents retrieved   |\n",
    "            +-------------------------------+\n",
    "```\n",
    "\n",
    "### Preparazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5c20ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "C:\\Users\\felip\\AppData\\Local\\Temp\\ipykernel_15488\\3917626230.py:17: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders.directory import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "loader = DirectoryLoader(\"./data\", glob=\"**/*.txt\")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# nel vectorstore memorizziamo i piccoli chunks che poi andranno retrievati\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"full_documents\", embedding_function=OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bcacf2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üíæ LangChain In-Memory DocStore\n",
    "\n",
    "Recuperati i piccoli chunks dal vectorstore si cercano gli IDs dei genitori con i quale recuperiamo apppunto i documenti pi√π grandi (genitori) dal InMemoryStore\n",
    "\n",
    "* Classe `InMemoryDocstore`\n",
    "* Utile per **prototipazione**, ma **non persistente**\n",
    "* Se riavvii il server, **perdi tutti i documenti**\n",
    "\n",
    "### Uso base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dacb9f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdcc971",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß™ Differenza tra Splitter Genitore e Figlio\n",
    "\n",
    "* **Parent Splitter**: divide i documenti in **blocchi medi**, da salvare nel `docstore`\n",
    "* **Child Splitter**: divide ogni blocco genitore in **pezzi piccoli**, da salvare nel `vectorstore`\n",
    "\n",
    "> Lo splitter padre √® **opzionale**, lo splitter figlio √® **obbligatorio**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a9e9c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "docstore = InMemoryStore()\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=250)\n",
    "\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=600)\n",
    "\n",
    "# con ParentDocumentRetriever() splittiamo i documenti genitori prima \n",
    "# quindi la dimensione dei chunks deve essere pi√π grande della dimensione dei chunks figli \n",
    "# supponiamo di avere tre documenti .txt, e che poi lo splitter genitore divida i documenti in 10 chunks\n",
    "# e poi uno splitter figlio che crea chunks ancora pi√π piccoli, ad esempio in 30 chunks\n",
    "\n",
    "# facciamo una prova prima senza il parent_splitter\n",
    "\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore, \n",
    "    docstore=docstore,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter\n",
    ")\n",
    "\n",
    "retriever.add_documents(docs, ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ff42787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# usiamo il metodo yield_keys() per ricavare dal docstore le chiavi \n",
    "# e vedere quanti documenti abbiamo al suo interno\n",
    "\n",
    "len(list(docstore.yield_keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c6e339",
   "metadata": {},
   "source": [
    "Quindi abbiamo 3 documenti .txt, 22 chuks pi√π grandi nel docstore, e ancora di pi√π chunks pi√π piccoli nel vectorstore su cui eseguiamo le nostre query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86c00a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data\\\\restaurant.txt'}, page_content=\"One evening, as the sun cast a golden glow over the city, a renowned food critic, Elena Rossi, stepped into Chef Amico. Her mission was to uncover the secret behind the restaurant's growing fame. She was greeted by Amico himself, whose eyes sparkled with the joy of a man who loved his work.\"),\n",
       " Document(metadata={'source': 'data\\\\founder.txt'}, page_content='As he grew, so did his desire to explore beyond the shores of Sicily. Venturing through Italy, Amico worked alongside renowned chefs, each teaching him a new facet of Italian cuisine. From the rolling hills of Tuscany to the romantic canals of Venice, he absorbed the diverse regional flavors, techniques, and traditions that would later influence his unique culinary style.\\n\\nCreating Chef Amico‚Äôs Restaurant'),\n",
       " Document(metadata={'source': 'data\\\\founder.txt'}, page_content='Creating Chef Amico‚Äôs Restaurant\\n\\nReturning to Palermo with a vision, Amico opened the doors to \"Chef Amico,\" a restaurant that was a culmination of his travels and a tribute to his Sicilian roots. Nestled in a quaint corner of the city, the restaurant quickly gained fame for its authentic flavors and Amico‚Äôs innovative twists on traditional recipes.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"who is the owner?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6fc70c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üö´ Limiti della memoria volatile\n",
    "\n",
    "* ‚ùå `InMemoryStore` non √® adatto a produzione\n",
    "* ‚úÖ Serve un sistema **persistente**, ad esempio **PostgreSQL**\n",
    "\n",
    "---\n",
    "\n",
    "## üß± Creare un DocStore personalizzato con PostgreSQL\n",
    "\n",
    "### 1. üîñ Chiavi e metadati\n",
    "\n",
    "* Ogni documento ha una `doc_id` nei metadati\n",
    "* LangChain si aspetta che il `docstore` gestisca una mappa chiave ‚Üí valore\n",
    "\n",
    "### 2. üì¶ Serializzazione\n",
    "\n",
    "* I documenti vengono **serializzati** in JSON per essere salvati\n",
    "* Alla lettura, il JSON viene **deserializzato** in `Document`\n",
    "\n",
    "---\n",
    "\n",
    "## üßæ Struttura della tabella Postgres\n",
    "\n",
    "```sql\n",
    "CREATE TABLE documents (\n",
    "    key TEXT PRIMARY KEY,\n",
    "    value JSONB NOT NULL\n",
    ");\n",
    "```\n",
    "\n",
    "* `key` ‚Üí identificatore univoco del documento (`doc_id`)\n",
    "* `value` ‚Üí JSON serializzato del documento LangChain\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Implementare il DocStore personalizzato\n",
    "\n",
    "### Requisiti minimi (ereditando da `BaseStore`):\n",
    "\n",
    "* `mget(keys: List[str]) ‚Üí Dict[str, Any]`\n",
    "* `mset(kv_pairs: Dict[str, Any]) ‚Üí None`\n",
    "* `yield_keys() ‚Üí Iterator[str]`\n",
    "\n",
    "Optional async versions:\n",
    "\n",
    "* `amget`, `amset`, `amdelete`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61e527b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creiamo il nostro modello di documento\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "# proprtio come i Document di Langchain ma con io campo key in pi√π \n",
    "class DocumentModel(BaseModel):\n",
    "    key: Optional[str] = Field(None)\n",
    "    page_content: Optional[str] = Field(None)\n",
    "    metadata: dict = Field(default_factory=dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781507d5",
   "metadata": {},
   "source": [
    "- la tabella consiste di sole due colonne key e value\n",
    "- la colonna key √® una stringa ed la primary key della tabella \n",
    "- la colonna value √® il JSON Binary \n",
    "- abbiamo una lista di Document che saranno convertiti in un oggetto JSON dato che non \n",
    "- √® possibile memorizzare direttamente le classi Python in un database, tale processo √® chimaato serializzazione \n",
    "- qundo recuperiamo i dati dal database convertiamo l'oggetto JSON in una classe Document, quindi eseguiamo la deserializzazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd24938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creiamo la nostra tabella per sql\n",
    "\n",
    "from sqlalchemy import Column, String, create_engine\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy.dialects.postgresql import JSONB\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class SQLDocument(Base):\n",
    "    __tablename__ = \"docstore\"\n",
    "    key = Column(String, primary_key=True)\n",
    "    value = Column(JSONB)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<SQLDocument(key='{self.key}', value='{self.value}')>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5f30cb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## üß™ Ricapitolando il flusso\n",
    "\n",
    "```text\n",
    "1. Documenti grandi ‚Üí splittati in chunk genitori\n",
    "2. Chunk genitori ‚Üí salvati nel Postgres docstore\n",
    "3. Chunk figli ‚Üí derivati da quelli genitori ‚Üí salvati nel vector store\n",
    "4. Retrieval ‚Üí su embedding dei figli\n",
    "5. Lookup ‚Üí doc_id ‚Üí carica documento completo dal Postgres docstore\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
