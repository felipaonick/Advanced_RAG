{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41dfed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creiamo il nostro modello di documento\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "# proprtio come i Document di Langchain ma con io campo key in pi√π \n",
    "class DocumentModel(BaseModel):\n",
    "    key: Optional[str] = Field(None)\n",
    "    page_content: Optional[str] = Field(None)\n",
    "    metadata: dict = Field(default_factory=dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4989595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creiamo la nostra tabella per sql\n",
    "\n",
    "from sqlalchemy import Column, String, create_engine\n",
    "from sqlalchemy.orm import declarative_base\n",
    "from sqlalchemy.dialects.postgresql import JSONB\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class SQLDocument(Base):\n",
    "    __tablename__ = \"docstore\"\n",
    "    key = Column(String, primary_key=True)\n",
    "    value = Column(JSONB)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<SQLDocument(key='{self.key}', value='{self.value}')>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddcb1ea",
   "metadata": {},
   "source": [
    "# üß© Costruire un DocStore Personalizzato con PostgreSQL in LangChain\n",
    "\n",
    "## üìå Obiettivo\n",
    "\n",
    "Creare un sistema **persistente** per salvare i documenti ‚Äúparent‚Äù recuperabili con il **Parent-Child Retriever**, evitando il limite dell‚Äô`InMemoryDocstore`.\n",
    "\n",
    "---\n",
    "\n",
    "## üß± 1. Creazione del `PostgresStore`\n",
    "\n",
    "### ‚úÖ Infrastruttura\n",
    "\n",
    "* **Eredita** da `BaseStore[str, Document]`\n",
    "* Funziona con:\n",
    "\n",
    "  * `key: str`\n",
    "  * `value: Document` (serializzato in JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f20f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "from typing import Generic, Iterator, Sequence, TypeVar\n",
    "from langchain.schema import Document \n",
    "from langchain_core.stores import BaseStore\n",
    "\n",
    "from sqlalchemy.orm import sessionmaker, scoped_session\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "D = TypeVar(\"D\", bound=Document)\n",
    "\n",
    "class PostgresStore(BaseStore[str, DocumentModel], Generic[D]):\n",
    "    def __init__(self, connection_string: str):\n",
    "        self.engine = create_engine(connection_string)\n",
    "        Base.metadata.create_all(self.engine)\n",
    "        self.Session = scoped_session(sessionmaker(bind=self.engine))\n",
    "\n",
    "    def serialize_document(self, doc: Document) -> dict:\n",
    "        return {\"page_content\": doc.page_content, \"metadata\": doc.metadata}\n",
    "\n",
    "    def deserialize_document(self, value: dict) -> Document:\n",
    "        return Document(\n",
    "            page_content=value.get(\"page_content\", \"\"),\n",
    "            metadata=value.get(\"metadata\", {})\n",
    "        )\n",
    "\n",
    "    # usato per prendere i documenti\n",
    "    def mget(self, keys: Sequence[str]) -> list[Document]:\n",
    "        with self.Session() as session:\n",
    "            try: \n",
    "                sql_documents = (\n",
    "                    session.query(SQLDocument).filter(SQLDocument.key.in_(keys)).all()\n",
    "                )\n",
    "\n",
    "                return [\n",
    "                    self.deserialize_document(sql_doc.value)\n",
    "                    for sql_doc in sql_documents\n",
    "                ]\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in mget: {e}\")\n",
    "                session.rollback()\n",
    "                return []\n",
    "\n",
    "    # metodo per l'inserimento dei dati nel nostro docstore\n",
    "    def mset(self, key_value_pairs: Sequence[tuple[str, Document]]) -> None:\n",
    "        with self.Session() as session:\n",
    "            try:\n",
    "                serialized_docs = []\n",
    "                for key, document in key_value_pairs:\n",
    "                    serialized_doc = self.serialize_document(document)\n",
    "                    serialized_docs.append((key, serialized_doc))\n",
    "\n",
    "                # in realt√† abbiamo bisogno di un elenco di documenti SQL\n",
    "                documents_to_update = [\n",
    "                    SQLDocument(key=key, value=value) for key, value in serialized_docs\n",
    "                ]\n",
    "\n",
    "                session.bulk_save_objects(documents_to_update, update_changed_only=True)\n",
    "\n",
    "                session.commit()\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in mset: {e}\")\n",
    "                session.rollback()\n",
    "\n",
    "    # metodo per cacellare dal docstore\n",
    "    def mdelete(self, keys: Sequence[str]) -> None:\n",
    "        with self.Session() as session:\n",
    "            try:\n",
    "                session.query(SQLDocument).filter(SQLDocuments.key.in_(keys)).delete(\n",
    "                    synchronize_session=False\n",
    "                )\n",
    "                session.commit()\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in mdelete: {e}\")\n",
    "                session.roll()\n",
    "\n",
    "\n",
    "    def yield_keys(self) -> Iterator[str]:\n",
    "        with self.Session() as session:\n",
    "            try:\n",
    "                query = session.query(SQLDocument.key)\n",
    "                for key in query:\n",
    "                    yield key[0]\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in yield_keys: {e}\")\n",
    "                session.rollback()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c141d69",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üê≥ 4. Setup PostgreSQL + PGVector via Docker Compose\n",
    "\n",
    "### ‚úÖ Parametri\n",
    "\n",
    "```yaml\n",
    "POSTGRES_DB: vector_db\n",
    "POSTGRES_USER: admin\n",
    "POSTGRES_PASSWORD: admin\n",
    "ports:\n",
    "  - \"45432:5432\"\n",
    "```\n",
    "\n",
    "Verifica che tutto funzioni:\n",
    "\n",
    "```bash\n",
    "docker ps        # controllo container\n",
    "docker compose up\n",
    "```\n",
    "\n",
    "Ora che il container √® pronto possiamo connetterci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4a97b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.pgvector import PGVector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "DATABASE_URL = \"postgresql+psycopg2://admin:admin@localhost:5432/vectordb\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "store = PGVector( # vectorstore di Postgres\n",
    "    collection_name=\"vectordb\",\n",
    "    connection_string=DATABASE_URL,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f03efc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ 5. Integrazione con Parent-Child Retriever\n",
    "\n",
    "\n",
    "Ora possiamo utilizzare il ParentDocumentRetriever passando il vectorstore appena creato, e il nostro docstore custom `PostgresStore()` il quale prende in input la stringa di connessione, la quale si tratta dello stesso URL del PGVector\n",
    "\n",
    "```python\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=pgvector,\n",
    "    docstore=PostgresStore(conn_str),\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=optional_parent_splitter\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06224ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders.directory import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "loader = DirectoryLoader(\"./data\", glob=\"**/*.txt\")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef917cc2",
   "metadata": {},
   "source": [
    "‚úÖ Se ids=None:\n",
    "LangChain:\n",
    "\n",
    "genera ID univoci automaticamente per ogni documento\n",
    "\n",
    "solitamente usa uuid.uuid4().hex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aee6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=250)\n",
    "\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=600)\n",
    "\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=store,\n",
    "    docstore=PostgresStore(connection_string=DATABASE_URL),\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter\n",
    ")\n",
    "\n",
    "\n",
    "# recupera i documenti parent (con contesto pi√π ampio) dal docstore\n",
    "retriever.add_documents(docs, ids=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6974d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data\\\\founder.txt'}, page_content='As he grew, so did his desire to explore beyond the shores of Sicily. Venturing through Italy, Amico worked alongside renowned chefs, each teaching him a new facet of Italian cuisine. From the rolling hills of Tuscany to the romantic canals of Venice, he absorbed the diverse regional flavors, techniques, and traditions that would later influence his unique culinary style.\\n\\nCreating Chef Amico‚Äôs Restaurant'),\n",
       " Document(metadata={'source': 'data\\\\founder.txt'}, page_content='Creating Chef Amico‚Äôs Restaurant\\n\\nReturning to Palermo with a vision, Amico opened the doors to \"Chef Amico,\" a restaurant that was a culmination of his travels and a tribute to his Sicilian roots. Nestled in a quaint corner of the city, the restaurant quickly gained fame for its authentic flavors and Amico‚Äôs innovative twists on traditional recipes.'),\n",
       " Document(metadata={'source': 'data\\\\restaurant.txt'}, page_content=\"One evening, as the sun cast a golden glow over the city, a renowned food critic, Elena Rossi, stepped into Chef Amico. Her mission was to uncover the secret behind the restaurant's growing fame. She was greeted by Amico himself, whose eyes sparkled with the joy of a man who loved his work.\")]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"who is the owner?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd7d532",
   "metadata": {},
   "source": [
    "Dato che PGVector (vectostore) √® basato su Postgres, non dobbiamo trattarlo come una scatola nera, ma possiamo eseguire query sul database. Sono stati creati due script in python per ispezionare il database e per eliminare le tabelle presenti. Gli script sono `inspect_db.py` e `clear_tables.py`\n",
    "\n",
    "```bash\n",
    "<comando> python inspect_db.py\n",
    "Table 'products' not found.\n",
    "Table 'langchain_pg_embedding' has 98 rows.  # questi sono i child_splitter\n",
    "Table 'docstore' has 22 rows. # questi sono i parent_splitter \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ba11e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Verifica Effettiva\n",
    "\n",
    "| Configurazione          | Righe VectorStore | Righe DocStore |\n",
    "| ----------------------- | ----------------- | -------------- |\n",
    "| Solo splitter figlio    | 93                | 0              |\n",
    "| Splitter padre + figlio | 98                | 22             |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Quando usare il Parent-Child Retrieval?\n",
    "\n",
    "### ‚úÖ Raccomandato se:\n",
    "\n",
    "* Usi un **LLM con finestra di contesto ampia**\n",
    "* I documenti trattano **argomenti multipli o complessi**\n",
    "* Vuoi **preservare il contesto semantico**, ma migliorare il matching semantico\n",
    "\n",
    "### üö´ Non necessario se:\n",
    "\n",
    "* I documenti sono gi√† **monotematici**\n",
    "* Hai un **ottimo chunking LLM-based**\n",
    "* Vuoi evitare la **complessit√†** del parent-child retrieval\n",
    "\n",
    "---\n",
    "\n",
    "## üìé Conclusione\n",
    "\n",
    "Hai costruito:\n",
    "\n",
    "* Un `PostgresStore` persistente\n",
    "* Integrato con `ParentDocumentRetriever`\n",
    "* Completamente compatibile con LangChain\n",
    "* Ispezionabile e gestibile grazie a SQL\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
