{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3087a90e",
   "metadata": {},
   "source": [
    "# üõ°Ô∏è Introduzione a **Nemo Guardrails** ‚Äì Sicurezza per LLM Conversazionali\n",
    "\n",
    "## üí° Cos'√® Nemo Guardrails?\n",
    "\n",
    "**Nemo Guardrails** √® un toolkit **open source** progettato per:\n",
    "\n",
    "* Aggiungere **guardrail programmabili** (binari di sicurezza) alle applicazioni **LLM-based**.\n",
    "* Proteggere da comportamenti indesiderati, **jailbreak**, **iniezioni di prompt**, output inappropriati, ecc.\n",
    "\n",
    "### üéØ Obiettivi principali:\n",
    "\n",
    "| Obiettivo                   | Descrizione                                                       |\n",
    "| --------------------------- | ----------------------------------------------------------------- |\n",
    "| ‚úÖ Affidabilit√†              | Controllo sull‚Äôoutput dell‚ÄôLLM in contesti critici                |\n",
    "| üõ°Ô∏è Sicurezza               | Protezione contro attacchi (es. SQL injection, jailbreak)         |\n",
    "| üß† Guidare le conversazioni | Definire **flussi conversazionali** chiari e tracciabili          |\n",
    "| üîÑ Standardizzazione        | Applicare best practices, SOP, autenticazioni, percorsi specifici |\n",
    "\n",
    "---\n",
    "\n",
    "## üß∞ Componenti principali di Nemo Guardrails\n",
    "\n",
    "### üß± 1. `Colang` ‚Äì Language per il design conversazionale\n",
    "\n",
    "* Linguaggio dichiarativo per **modellare conversazioni**.\n",
    "* Concetti chiave: `messages` e `flows`.\n",
    "\n",
    "### üìÑ 2. YAML ‚Äì Configurazione dei modelli\n",
    "\n",
    "* Imposta il modello da usare (es. GPT-3.5-Turbo)\n",
    "* Specifica temperature, max\\_tokens, ecc.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Esempio: Flusso di saluto\n",
    "\n",
    "### üîπ Messaggio dell‚Äôutente (`colang`)\n",
    "\n",
    "```colang\n",
    "define user express greeting\n",
    "  \"hello\"\n",
    "  \"hi\"\n",
    "\n",
    "define bot express greeting\n",
    "  \"Hello. How can I help you today?\"\n",
    "```\n",
    "\n",
    "### üîπ Flusso conversazionale (`colang`)\n",
    "\n",
    "```colang\n",
    "flow hello:\n",
    "  user express greeting\n",
    "  bot express greeting\n",
    "```\n",
    "\n",
    "### üîπ Configurazione YAML\n",
    "\n",
    "```yaml\n",
    "models:\n",
    "  - type: openai\n",
    "    engine: gpt-3.5-turbo\n",
    "    api_key: ${OPENAI_API_KEY}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üßë‚Äçüíª Implementazione in Jupyter Notebook\n",
    "\n",
    "### üîß Setup iniziale\n",
    "\n",
    "Dobbiamo modificare il ciclo asyncIO per far funzionare NeMo guardrails in un ambiente di Jupyter Notebook.\n",
    "\n",
    "```python\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "```\n",
    "\n",
    "## üîÑ Variabili e logica condizionale (if / else)\n",
    "\n",
    "### üîπ Nuovo colang con variabili\n",
    "\n",
    "```colang\n",
    "define user express greeting\n",
    "  \"hello\"\n",
    "  \"hi\"\n",
    "\n",
    "define bot express normal greeting\n",
    "  \"Hello. How can I help you today?\"\n",
    "\n",
    "define bot express personal greeting\n",
    "  \"Hello $username. It's nice to see you again.\"\n",
    "```\n",
    "\n",
    "### üîπ Flusso condizionale\n",
    "\n",
    "```colang\n",
    "flow hello:\n",
    "  user express greeting\n",
    "  if $username:\n",
    "    bot express personal greeting\n",
    "  else:\n",
    "    bot express normal greeting\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f63dbe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef32b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edab2d78",
   "metadata": {},
   "source": [
    "**Colang** √® un linguaggio di modellazione per applicazioni conversazionali\n",
    "\n",
    "Utilizziamo Colang per progettare il modo in cui deve avvenire la conversazione tra un utente e un bot.\n",
    "\n",
    "In Colang i due concetti fondamentali sono i **messaggi** e i **flussi**\n",
    "\n",
    "Una conversazione viene modellata come uno scambio di un messaggio dell'utente e di un messaggio del bot.\n",
    "\n",
    "Per farlo funzionare dobbiamo anche definire il `yaml_content` che definisce il modello che useremo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a04c5cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# potrebbero essre anche file YAML\n",
    "\n",
    "colang_content = \"\"\"\n",
    "define user express greeting\n",
    "    \"hello\"\n",
    "    \"hi\"\n",
    "    \n",
    "define bot express greeting\n",
    "    \"Hello there!! Can I help you today?\"\n",
    "    \n",
    "define flow hello\n",
    "    user express greeting\n",
    "    bot express greeting\n",
    "\"\"\"\n",
    "\n",
    "yaml_content = \"\"\"\n",
    "models:\n",
    "- type: main\n",
    "  engine: openai\n",
    "  model: gpt-3.5-turbo\n",
    "\"\"\"\n",
    "\n",
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "config = RailsConfig.from_content(\n",
    "    yaml_content=yaml_content, colang_content=colang_content\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03141682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\felip\\Desktop\\Advanced_RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 5 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:17<00:00,  3.55s/it]\n"
     ]
    }
   ],
   "source": [
    "rails = LLMRails(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e542d64",
   "metadata": {},
   "source": [
    "\n",
    "### üîÅ Generazione della risposta\n",
    "\n",
    "```python\n",
    "response = await rails.generate_async(\"hello\")\n",
    "print(response)\n",
    "# Output: Hello. How can I help you today?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9c411b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there!! Can I help you today?\n"
     ]
    }
   ],
   "source": [
    "res = await rails.generate_async(prompt=\"Hello\")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86db257b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß© Modalit√† conversazione (multi-turn)\n",
    "\n",
    "Invece di passare solo un prompt, possiamo anche passare una conversazione completa simile a come facciamo con le OpenAI API.\n",
    "\n",
    "```python\n",
    "await rails.generate_async([\n",
    "    {\"role\": \"user\", \"content\": \"hello\"}\n",
    "])\n",
    "```\n",
    "\n",
    "### ‚è±Ô∏è Output:\n",
    "\n",
    "```json\n",
    "{\"role\": \"assistant\", \"content\": \"Hello. How can I help you today?\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea81e67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'Hello there!! Can I help you today?'}\n"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"Hey there!\"}]\n",
    "\n",
    "res = await rails.generate_async(messages=messages)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719a78b2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Possiamo rendere il Colang un po' pi√π complesso\n",
    "\n",
    "Possiamo usare anche variabili per rendere il tutto pi√π dinamico.\n",
    "\n",
    "Usaimo la variabile $username per indicare che il saluto personale deve essere rivolto ad un user in particolare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6350bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "colang_content = \"\"\"\n",
    "define user express greeting\n",
    "    \"hello\"\n",
    "    \"hi\"\n",
    "\n",
    "define bot express greeting\n",
    "    \"Hello there!! Can I help you today?\"\n",
    "\n",
    "define bot personal greeting\n",
    "    \"Hello $username, nice to see you again!\"\n",
    "\n",
    "define flow hello\n",
    "    user express greeting\n",
    "    if $username\n",
    "        bot personal greeting\n",
    "    else\n",
    "        bot express greeting\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "460e7615",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RailsConfig.from_content(\n",
    "    yaml_content=yaml_content, colang_content=colang_content\n",
    ")\n",
    "\n",
    "rails = LLMRails(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b9621a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'Hello there!! Can I help you today?'}\n"
     ]
    }
   ],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"Hey there!\"}]\n",
    "\n",
    "res = await rails.generate_async(messages=messages)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8195e8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'Hello Markus, nice to see you again!'}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"context\", \"content\": {\"username\": \"Markus\"}},\n",
    "    {\"role\": \"user\", \"content\": \"Hey there!\"}\n",
    "]\n",
    "\n",
    "res = await rails.generate_async(messages=messages)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb762c1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß± Riepilogo dei concetti chiave\n",
    "\n",
    "| Concetto         | Descrizione                                                 |\n",
    "| ---------------- | ----------------------------------------------------------- |\n",
    "| `colang`         | Linguaggio per definire esempi e flussi conversazionali     |\n",
    "| `flow`           | Logica condizionale delle conversazioni                     |\n",
    "| `YAML`           | Configura i modelli da usare e i parametri                  |\n",
    "| `RailsConfig`    | Oggetto che unisce YAML + Colang                            |\n",
    "| `LLMRails`       | Motore per eseguire i guardrail                             |\n",
    "| `generate_async` | Metodo asincrono per generare output con i vincoli definiti |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Conclusione\n",
    "\n",
    "Hai imparato a:\n",
    "\n",
    "* Usare **Nemo Guardrails** per controllare l‚Äôoutput del tuo chatbot.\n",
    "* Modellare una conversazione con `colang`.\n",
    "* Eseguire logiche condizionali e usare variabili di contesto.\n",
    "* Creare un sistema LLM pi√π **robusto, sicuro e prevedibile**.\n",
    "\n",
    "> üîú Nella prossima lezione imparerai a **integrare Nemo Guardrails con LangChain** per un controllo conversazionale ancora pi√π potente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
