{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7127c4bd",
   "metadata": {},
   "source": [
    "# üß© **LangChain Expression Language (LCEL) ‚Äî Deep Dive**\n",
    "\n",
    "## üéì **Cos‚Äô√® LCEL**\n",
    "\n",
    "* LCEL (**LangChain Expression Language**) √® una **sintassi dichiarativa** per **costruire catene** di elaborazione dati.\n",
    "* Si ispira al comportamento **pipe** di **Linux**, dove l‚Äôoutput di un comando diventa l‚Äôinput del successivo.\n",
    "* In Python, LCEL realizza questo comportamento sfruttando il **sovraccarico di operatori** (`|`, `>>` ecc.).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è **Primo Esempio Pratico**\n",
    "\n",
    "### üìå **Obiettivo**\n",
    "\n",
    "Creare una **semplice catena**:\n",
    "\n",
    "1. Prompt ‚Üí\n",
    "2. Modello di chat (OpenAI) ‚Üí\n",
    "3. Parser di output.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Notebook di Esempio**\n",
    "\n",
    "* File: `00_lcel.ipynb`\n",
    "* Contiene tutti i passaggi per riprodurre la catena di base.\n",
    "\n",
    "---\n",
    "\n",
    "### üóÇÔ∏è **Componenti Importati**\n",
    "\n",
    "* `ChatOpenAI` ‚Üí Istanza del modello di chat.\n",
    "* `ChatPromptTemplate` ‚Üí Definizione del **prompt dinamico**.\n",
    "* `StrOutputParser` ‚Üí Parsing dell‚Äôoutput del modello in stringa.\n",
    "* Lettura **API Key** OpenAI da variabile d‚Äôambiente per eseguire il codice.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úèÔ∏è **Creazione del Prompt**\n",
    "\n",
    "```python\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}.\"\n",
    ")\n",
    "```\n",
    "\n",
    "* **Variabile:** `topic`\n",
    "* Esempio: `gelato`\n",
    "\n",
    "---\n",
    "\n",
    "## üîó **Creazione della Catena**\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "```\n",
    "\n",
    "* `|` = Operatore pipe sovraccaricato!\n",
    "* **Ordine:** prompt ‚Üí modello ‚Üí parser\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è **Esecuzione della Catena**\n",
    "\n",
    "```python\n",
    "response = chain.invoke({\"topic\": \"ice cream\"})\n",
    "print(response)\n",
    "```\n",
    "\n",
    "* **Output atteso:**\n",
    "  *‚ÄúWhy did the ice cream truck break down? Because it had too many scoops on board!‚Äù*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bb7583a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6339acf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "327eebbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['topic'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='tell me a short jike about {topic}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a short jike about {topic}\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b7cf00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the ice cream cone go to therapy?\\nBecause it wanted to find its true self-scoop!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86966d11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß© **Come Funziona Sotto il Cofano**\n",
    "\n",
    "1. **Sovraccarico Operatore Pipe (`|`)**\n",
    "\n",
    "   * LangChain implementa una **interfaccia `Runnable`**.\n",
    "   * Gli oggetti (`Prompt`, `LLM`, `Parser`) sono tutti `Runnable`.\n",
    "   * Il pipe crea una **sequenza di Runnables** concatenati.\n",
    "\n",
    "2. **Metodo `invoke`**\n",
    "\n",
    "   * Ogni `Runnable` ha `invoke()`.\n",
    "   * Puoi invocare:\n",
    "\n",
    "     * La **catena completa**.\n",
    "     * Ogni singolo step.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç **Esempio Interno**\n",
    "\n",
    "* Il **prompt** quando invocato restituisce:\n",
    "\n",
    "  ```python\n",
    "  {'messages': [HumanMessage(content=\"Tell me a short joke about ice cream.\")]}\n",
    "  ```\n",
    "\n",
    "* Il **modello** quando invocato genera:\n",
    "\n",
    "  ```python\n",
    "  {'messages': [AIMessage(content=\"Why did the ice cream truck...\")]}\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26fa0ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n"
     ]
    }
   ],
   "source": [
    "print(type(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8a2c827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='tell me a short jike about ice cream', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "736771b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the ice cream truck break down?\\nIt had too many \"licks\" and not enough \"cylinders\"!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 15, 'total_tokens': 41, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Bt8MQtcd9Nktn7YmxKpG3xUdyBxiB', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--3bf5362f-0589-4c0e-b73d-721ed43e4c9d-0', usage_metadata={'input_tokens': 15, 'output_tokens': 26, 'total_tokens': 41, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages.human import HumanMessage\n",
    "\n",
    "messages = [HumanMessage(content=\"tell me a short joke about ice cream\")]\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e910add8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚ö° **Ereditariet√† Classi**\n",
    "\n",
    "* `ChatOpenAI` ‚Üí `BaseChatModel` ‚Üí `BaseLanguageModel` ‚Üí `RunnableSerializable`.\n",
    "* Tutti gli step condividono la logica di esecuzione `invoke()` e `|`.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Key Takeaway**\n",
    "\n",
    "| üìù | **LCEL ti permette di combinare blocchi modulari (`Runnables`) in catene, eseguendoli in sequenza in modo semplice e leggibile.** |\n",
    "| -- | --------------------------------------------------------------------------------------------------------------------------------- |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ñ∂Ô∏è **Prossimo Passo**\n",
    "\n",
    "Nel prossimo modulo:\n",
    "\n",
    "* **Costruirai la tua versione personalizzata** di un‚Äôespressione LCEL.\n",
    "* Imparerai a definire operatori complessi e pipe dinamici.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4487ed",
   "metadata": {},
   "source": [
    "# üß† **Creare un LangChain Expression Language (LCEL) Personalizzato**\n",
    "\n",
    "## üß© Obiettivo\n",
    "\n",
    "Costruire una **versione semplificata e personalizzata** del comportamento LCEL, ovvero la **logica a pipeline (`|`)** usata da LangChain per concatenare operazioni.\n",
    "\n",
    "---\n",
    "\n",
    "## üß± **Concetti Chiave**\n",
    "\n",
    "* In Python, non esiste nativamente un operatore **pipe (`|`)** come in Linux.\n",
    "* LangChain **sovraccarica gli operatori** (`|`, `>>`, ecc.) per collegare componenti (prompt, modelli, parser).\n",
    "* Per replicarlo, creiamo una **classe base `Runnable`** e una **classe `RunnableSequence`**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîß **Passaggio 1: Creare la Classe Base `CRunnable`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "358de2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class CRunnable(ABC):\n",
    "    def __init__(self):\n",
    "        self.next = None\n",
    "\n",
    "    # decoratore della libreria abc\n",
    "    # indica che il metodo process deve essere obbligatoriamente \n",
    "    # implementato da ogni sottoclasse \n",
    "    @abstractmethod\n",
    "    def process(self, data):\n",
    "        \"\"\"\n",
    "        This method must be implemented by subclasses to define \n",
    "        data processing behavior.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def invoke(self, data):\n",
    "        processed_data = self.process(data)\n",
    "        if self.next is not None:\n",
    "            return self.next.invoke(processed_data)\n",
    "        return processed_data\n",
    "    \n",
    "    # metodo speciale che permette di sovraccaricare l'operatore | (pipe)\n",
    "    # a | b --> a.__or__(b)\n",
    "    # fa si che CRunnable supporti la sintassi obj1 | obj2\n",
    "    # che restituisce un oggetto CRunnableSequence contenente entrambi\n",
    "    def __or__(self, other):\n",
    "        return CRunnableSequence(self, other)\n",
    "    \n",
    "\n",
    "class CRunnableSequence(CRunnable):\n",
    "    def __init__(self, first, second):\n",
    "        super().__init__()\n",
    "        self.first = first\n",
    "        self.second = second\n",
    "\n",
    "    def process(self, data):\n",
    "        return data\n",
    "    \n",
    "    def invoke(self, data):\n",
    "        first_result = self.first.invoke(data)\n",
    "        return self.second.invoke(first_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f010650",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÅ **Passaggio 2: Costruire la Classe `CRunnableSequence`**\n",
    "\n",
    "```python\n",
    "class CRunnableSequence(CRunnable):\n",
    "    def __init__(self, first, second):\n",
    "        super().__init__()\n",
    "        self.first = first\n",
    "        self.second = second\n",
    "\n",
    "    def invoke(self, data):\n",
    "        # Esegue il primo runnable\n",
    "        result1 = self.first.invoke(data)\n",
    "        # Passa il risultato al secondo runnable\n",
    "        result2 = self.second.invoke(result1)\n",
    "        return result2\n",
    "\n",
    "    def process(self, data):\n",
    "        # Il metodo process non modifica nulla\n",
    "        return data\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è **Passaggio 3: Definire dei Runnable Personalizzati**\n",
    "\n",
    "### ‚ûï Aggiungi 10\n",
    "\n",
    "```python\n",
    "class AddTen(CRunnable):\n",
    "    def process(self, data):\n",
    "        return data + 10\n",
    "```\n",
    "\n",
    "### ‚úñÔ∏è Moltiplica per 2\n",
    "\n",
    "```python\n",
    "class MultiplyByTwo(CRunnable):\n",
    "    def process(self, data):\n",
    "        return data * 2\n",
    "```\n",
    "\n",
    "### üî§ Converti in Stringa\n",
    "\n",
    "```python\n",
    "class ToString(CRunnable):\n",
    "    def process(self, data):\n",
    "        return str(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89f6a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddTen(CRunnable):\n",
    "    def process(self, data):\n",
    "        print(\"AddTen: \", data)\n",
    "        return data +10\n",
    "    \n",
    "\n",
    "class MultiplyByTwo(CRunnable):\n",
    "    def process(self, data):\n",
    "        print(\"Multiply by 2: \", data)\n",
    "        return data * 2\n",
    "    \n",
    "\n",
    "class ConvertToString(CRunnable):\n",
    "    def process(self, data):\n",
    "        print(\"Convert to string: \", data)\n",
    "        return f\"Result: {data}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13bfe433",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = AddTen()\n",
    "b = MultiplyByTwo()\n",
    "c = ConvertToString()\n",
    "\n",
    "chain = a | b | c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53f8d747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AddTen:  10\n",
      "Multiply by 2:  20\n",
      "Convert to string:  40\n",
      "Result: 40\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke(10)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdb5b9b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîó **Passaggio 4: Comporre la Catena**\n",
    "\n",
    "```python\n",
    "chain = AddTen() | MultiplyByTwo() | ToString()\n",
    "```\n",
    "\n",
    "* Il `|` crea una sequenza eseguibile con pipe, come in LangChain.\n",
    "* Internamente esegue:\n",
    "\n",
    "  1. `data + 10`\n",
    "  2. `* 2`\n",
    "  3. `str()`\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ñ∂Ô∏è **Eseguire la Catena**\n",
    "\n",
    "```python\n",
    "result = chain.invoke(10)\n",
    "print(result)  # Output: \"40\"\n",
    "```\n",
    "\n",
    "üß† **Step-by-step:**\n",
    "\n",
    "* 10 + 10 = **20**\n",
    "* 20 √ó 2 = **40**\n",
    "* str(40) = **\"40\"**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Cosa Abbiamo Imparato**\n",
    "\n",
    "| Componente  | Funzione                                            |                                  |\n",
    "| ----------- | --------------------------------------------------- | -------------------------------- |\n",
    "| `CRunnable` | Interfaccia base per tutti i blocchi della catena   |                                  |\n",
    "| `__or__()`  | Sovraccarica \\`                                     | \\` per creare pipe tra Runnables |\n",
    "| `invoke()`  | Metodo pubblico per eseguire il pipeline            |                                  |\n",
    "| `process()` | Metodo da personalizzare per ogni step della catena |                                  |\n",
    "\n",
    "---\n",
    "\n",
    "## üèÜ **Conclusione**\n",
    "\n",
    "Hai costruito un **mini motore LCEL** personalizzato in Python!\n",
    "Con questo approccio puoi concatenare qualsiasi tipo di trasformazione in modo elegante e leggibile.\n",
    "\n",
    "---\n",
    "\n",
    "## üîú **Prossimo Modulo**\n",
    "\n",
    "Nel prossimo video esamineremo i **runnables pi√π importanti offerti da LangChain**, come:\n",
    "\n",
    "* `RunnableLambda`\n",
    "* `RunnablePassthrough`\n",
    "* `RunnableMap`\n",
    "* `RunnableSequence` (interno)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adae067",
   "metadata": {},
   "source": [
    "# üß† LangChain ‚Äî I Runnables Pi√π Importanti (Deep Dive)\n",
    "\n",
    "## üìö Cos‚Äô√® un Runnable?\n",
    "\n",
    "Un `Runnable` √® una **unit√† componibile ed eseguibile** all'interno di LangChain. Ogni oggetto `Runnable` espone un'interfaccia standard (`invoke`, `stream`, `batch`, ecc.) ed √® compatibile con l‚Äô**operatore `|`** per creare pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ `RunnablePassthrough`\n",
    "\n",
    "### üîπ Descrizione\n",
    "\n",
    "Un **Runnable che non modifica l‚Äôinput**: lo passa semplicemente al passo successivo.\n",
    "\n",
    "### üî∏ Uso tipico\n",
    "\n",
    "Utile per debugging o all'interno di `RunnableParallel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "154c1678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = RunnablePassthrough() | RunnablePassthrough() | RunnablePassthrough()\n",
    "\n",
    "chain.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7a43db",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßÆ `RunnableLambda`\n",
    "\n",
    "### üîπ Descrizione\n",
    "\n",
    "Permette di **inserire una funzione Python arbitraria** all‚Äôinterno di una catena `Runnable`.\n",
    "\n",
    "### üî∏ Esempio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56ebf1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_to_upper(input: str):\n",
    "    output = input.upper()\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80ebc5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HELLO'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "chain = RunnablePassthrough() | RunnableLambda(input_to_upper) | RunnablePassthrough()\n",
    "chain.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da39c4b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üåø `RunnableParallel`\n",
    "\n",
    "### üîπ Descrizione\n",
    "\n",
    "Esegue pi√π Runnables **in parallelo** (non concorrente), creando un dizionario con pi√π risultati.\n",
    "\n",
    "### üî∏ Uso base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7384a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 'hello', 'y': 'hello'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "chain = RunnableParallel({\"x\": RunnablePassthrough(), \"y\": RunnablePassthrough()})\n",
    "\n",
    "chain.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a83afeb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': {'input': 'hello', 'input2': 'goodbye'},\n",
       " 'y': {'input': 'hello', 'input2': 'goodbye'}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"hello\", \"input2\": \"goodbye\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2011561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': {'input': 'hello', 'input2': 'goodbye'}, 'y': 'goodbye'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnableParallel({\"x\": RunnablePassthrough(), \"y\": lambda z: z[\"input2\"]})\n",
    "\n",
    "chain.invoke({\"input\": \"hello\", \"input2\": \"goodbye\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e2bd3d",
   "metadata": {},
   "source": [
    "## Nested Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27d2c104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_keys_to_uppercase(input: dict):\n",
    "    output = input.get(\"input\", \"not found\").upper()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64745109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 'HELLO', 'y': 'goodbye'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnableParallel({\"x\": RunnablePassthrough() | RunnableLambda(find_keys_to_uppercase), \n",
    "                          \"y\": lambda z: z[\"input2\"]})\n",
    "\n",
    "\n",
    "chain.invoke({\"input\": \"hello\", \"input2\": \"goodbye\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d5ae53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableParallel({\"x\": RunnablePassthrough()})\n",
    "\n",
    "def assign_func(_):\n",
    "    return 100\n",
    "\n",
    "def multiply(input):\n",
    "    return input * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05cf5daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': {'input': 'hello', 'input2': 'goodbye'}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"hello\", \"input2\": \"goodbye\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb68e3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': {'input': 'hello', 'input2': 'goodbye'}, 'extra': 100}\n"
     ]
    }
   ],
   "source": [
    "chain = RunnableParallel({\"x\": RunnablePassthrough()}).assign(extra=RunnableLambda(assign_func))\n",
    "\n",
    "result = chain.invoke({\"input\": \"hello\", \"input2\": \"goodbye\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1736f085",
   "metadata": {},
   "source": [
    "## Combine multiple chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "755fa681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor(input: dict):\n",
    "    return input.get(\"extra\", \"Key not found\")\n",
    "\n",
    "\n",
    "def cupper(upper: str):\n",
    "    return str(upper).upper()\n",
    "\n",
    "new_chain = RunnableLambda(extractor) | RunnableLambda(cupper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7feb728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_chain.invoke({\"extra\": \"test\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a317c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'100'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain = chain | new_chain\n",
    "\n",
    "final_chain.invoke({\"input\": \"hello\", \"input2\": \"goodbye\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f92e5d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Punti Chiave\n",
    "\n",
    "| Runnable              | Scopo principale                                       |\n",
    "| --------------------- | ------------------------------------------------------ |\n",
    "| `RunnablePassthrough` | Passa l‚Äôinput senza modificarlo                        |\n",
    "| `RunnableLambda`      | Avvolge funzioni Python arbitrarie                     |\n",
    "| `RunnableParallel`    | Crea output paralleli da uno stesso input              |\n",
    "| `RunnableAssign`      | Aggiunge o sovrascrive chiavi in un dizionario d'input |\n",
    "\n",
    "---\n",
    "\n",
    "## üîú Prossima Lezione\n",
    "\n",
    "Nel prossimo modulo:\n",
    "‚û°Ô∏è **Esempi reali** di pipeline RAG con LangChain, combinando `LCEL`, retrieval, parsing e valutazione."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6b9450",
   "metadata": {},
   "source": [
    "# üöÄ **Real-World RAG Pipeline con LangChain e LCEL**\n",
    "\n",
    "## üß† Obiettivo della Lezione\n",
    "\n",
    "Costruire una **pipeline RAG** reale usando:\n",
    "\n",
    "* Prompt dinamici\n",
    "* Modello `ChatOpenAI`\n",
    "* Retriever basato su `Chroma`\n",
    "* Embedding con `OpenAIEmbeddings`\n",
    "* Output parser\n",
    "* LCEL (LangChain Expression Language)\n",
    "\n",
    "---\n",
    "\n",
    "## üîê Setup Iniziale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cfe207a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f681f34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[HumanMessage(content='Tell me an interesting fact about dog', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Tell me an interesting fact about {topic}\")\n",
    "\n",
    "prompt_val = prompt.invoke({\"topic\": \"dog\"})\n",
    "\n",
    "print(prompt_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e209b354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Tell me an interesting fact about dog', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(prompt_val.to_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e63f0733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Dogs have an incredible sense of smell, with some breeds being able to detect scents up to 100,000 times better than humans. This ability has led to them being used in a variety of jobs such as search and rescue, bomb detection, and even medical scent detection to detect illnesses like cancer.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 14, 'total_tokens': 76, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BtAszOhBoGxnOFleDI0EIw04RGc7G', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--e855543a-5000-48d8-b356-55558e79e7fc-0', usage_metadata={'input_tokens': 14, 'output_tokens': 62, 'total_tokens': 76, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "result = model.invoke(prompt_val)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0dd7a8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dogs have an incredible sense of smell, with some breeds being able to detect scents up to 100,000 times better than humans. This ability has led to them being used in a variety of jobs such as search and rescue, bomb detection, and even medical scent detection to detect illnesses like cancer.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "output_parser.invoke(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b43eb6",
   "metadata": {},
   "source": [
    "## Now let's do this LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "10ff3061",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Tell me an interesting fact about {topic}\")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "basicchain = model | output_parser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f2ba560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basicchain.invoke(\"hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57ad599b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dogs have an incredible sense of smell, with some breeds being able to detect scents at concentrations as low as one part per trillion. This makes them invaluable in search and rescue operations, detecting drugs and explosives, and even diagnosing medical conditions such as cancer in humans.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"dog\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679853ca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß± Costruzione Prompt RAG\n",
    "\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the question based only on the context below.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\"\n",
    ")\n",
    "```\n",
    "\n",
    "* **Variabili richieste**: `context`, `question`\n",
    "* üß© Il prompt costringe il modello a usare **solo il contesto recuperato**, non la conoscenza interna.\n",
    "\n",
    "---\n",
    "\n",
    "## üì• Input e Output\n",
    "\n",
    "| Step            | Input                               | Output                             |\n",
    "| --------------- | ----------------------------------- | ---------------------------------- |\n",
    "| Prompt Template | `{\"context\": ..., \"question\": ...}` | Lista di messaggi (`HumanMessage`) |\n",
    "| ChatOpenAI      | Lista messaggi                      | Messaggio `AIMessage`              |\n",
    "| StrOutputParser | Messaggio `AIMessage`               | `String` (solo testo)              |\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Embedding e Archivio Vettoriale\n",
    "\n",
    "```python\n",
    "from langchain.schema import Document\n",
    "\n",
    "documents = [\n",
    "    Document(page_content=\"Dogs love to eat meat and kibble.\"),\n",
    "    Document(page_content=\"Cats enjoy eating fish and chicken.\")\n",
    "]\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "```\n",
    "\n",
    "* I documenti vengono **embedded** e salvati localmente.\n",
    "* `retriever.invoke(query)` restituisce i documenti pi√π simili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8824a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma \n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"the dog loves to eat pizza\", metadata={\"source\": \"animal.txt\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"the cat loves to eat lasagna\", metadata={\"source\": \"animal.txt\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "db = Chroma.from_documents(docs, embedding_function)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40ab3d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felip\\AppData\\Local\\Temp\\ipykernel_15840\\2510795402.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retriever.get_relevant_documents(\"What does the dog want to eat?\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'animal.txt'}, page_content='the dog loves to eat pizza'),\n",
       " Document(metadata={'source': 'animal.txt'}, page_content='the cat loves to eat lasagna')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"What does the dog want to eat?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a19bfc97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'animal.txt'}, page_content='the dog loves to eat pizza'),\n",
       " Document(metadata={'source': 'animal.txt'}, page_content='the cat loves to eat lasagna')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What does the dog wnat to eat?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "993e8f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creiamo un semplice prompt per RAG\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dec7f34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\n",
    "        \"context\": (lambda x: x[\"question\"]) | retriever,\n",
    "        # \"question\": lambda x: x[\"question\"]\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b620a3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The dog loves to eat pizza.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke({\"question\": \"What does the dog like to eat?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b3ceeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# possiamo fare anche nel seguente modo\n",
    "# input come stringa\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "173c4d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The dog loves to eat pizza.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain.invoke(\"What does the dog like to eat?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bba4d1",
   "metadata": {},
   "source": [
    "‚úÖ Pi√π leggibile\n",
    "‚úÖ Comportamento identico (se il retriever accetta una `str`)\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Esecuzione\n",
    "\n",
    "```python\n",
    "response = rag_chain.invoke(\"What do dogs like to eat?\")\n",
    "print(response)\n",
    "# Output: Risposta generata basata sul contesto dei documenti\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Concetti Importanti\n",
    "\n",
    "| Concetto                | Spiegazione                                          |    |\n",
    "| ----------------------- | ---------------------------------------------------- | -- |\n",
    "| **Retriever**           | Converte query in embedding e trova documenti simili |    |\n",
    "| **Prompt Template**     | Usa `{context}` e `{question}` come segnaposto       |    |\n",
    "| **StrOutputParser**     | Estrae il contenuto del messaggio AI                 |    |\n",
    "| **RunnableLambda**      | Serve per manipolare input dinamicamente             |    |\n",
    "| **RunnablePassthrough** | Passa input invariato (es. `question`)               |    |\n",
    "| **LCEL Pipeline**       | Permette di collegare componenti con \\`              | \\` |\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Diagramma Semplificato\n",
    "\n",
    "```\n",
    "Input (stringa: domanda)\n",
    "    ‚îÇ\n",
    "    ‚îú‚îÄ‚îÄ context ‚îÄ‚îÄ> Retriever ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ                          ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ question ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "           ‚îÇ\n",
    "        Prompt Template\n",
    "           ‚îÇ\n",
    "        ChatOpenAI\n",
    "           ‚îÇ\n",
    "     StrOutputParser\n",
    "           ‚îÇ\n",
    "         Risposta\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîú Prossimo Step\n",
    "\n",
    "üìå **Integrazione della cronologia della chat** (chat history) nelle catene RAG:\n",
    "\n",
    "* Tenere conto del **contesto conversazionale**\n",
    "* Migliorare l‚Äôesperienza dei chatbot nel mondo reale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b509215",
   "metadata": {},
   "source": [
    "# üí¨ **Gestione della Cronologia delle Chat in LangChain (con RAG + LCEL)**\n",
    "\n",
    "## ‚ùì Il Problema\n",
    "\n",
    "* Nelle **chat multi-turno**, l'utente pu√≤ porre **domande dipendenti dal contesto**:\n",
    "\n",
    "  > Utente: Cosa piace mangiare ai cani?\n",
    "  > Bot: I cani amano la pizza.\n",
    "  > Utente: Davvero?\n",
    "\n",
    "* La domanda `\"Davvero?\"` non ha senso da sola.\n",
    "\n",
    "* Se eseguiamo una **vector search** solo su `\"Davvero?\"`, otterremo documenti **non pertinenti** ‚Üí output sbagliato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "12ab3964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import os\n",
    "\n",
    "\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"the dog loves to eat pizza\", metadata={\"source\": \"animal.txt\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"the cat loves to eat lasagna\", metadata={\"source\": \"animal.txt\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "db = Chroma.from_documents(docs, embedding_function)\n",
    "\n",
    "\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "14694c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'animal.txt'}, page_content='the dog loves to eat pizza'),\n",
       " Document(metadata={'source': 'animal.txt'}, page_content='the dog loves to eat pizza'),\n",
       " Document(metadata={'source': 'animal.txt'}, page_content='the cat loves to eat lasagna'),\n",
       " Document(metadata={'source': 'animal.txt'}, page_content='the cat loves to eat lasagna')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What exactly?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8a23c5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß† Soluzione: **Rewriting della domanda**\n",
    "\n",
    "üëâ LLM viene usato per **riscrivere la domanda** in modo autonomo (self-contained).\n",
    "\n",
    "### üìò Prompt di riscrittura\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e71c2db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "rephrase_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\n",
    "\"\"\"\n",
    "\n",
    "REPHRASE_TEMPLATE = PromptTemplate.from_template(rephrase_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dbb0ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "rephrase_chain = REPHRASE_TEMPLATE | ChatOpenAI(temperature=0) | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c43ae29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is that really what the dog likes to eat?'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rephrase_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"No, really?\",\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"What does the dog like to eat?\"),\n",
    "            AIMessage(content=\"Thuna!\")\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db8af0c",
   "metadata": {},
   "source": [
    "\n",
    "## üß± Catena di Recupero (RAG)\n",
    "\n",
    "### Prompt RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8f228d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîó Combinazione Finale: Rewriting + Retrieval + Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a106968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9585b908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | ANSWER_PROMPT\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a3e613cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = rephrase_chain | retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d8681580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, based on the provided context, it is stated that the dog loves to eat pizza.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"No, really?\",\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"What does the dog like to eat?\"),\n",
    "            AIMessage(content=\"Thuna!\")\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0af6dc3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß™ Chat with returning documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "403ef724",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_documents = {\"docs\": retriever, \"question\": RunnablePassthrough()}\n",
    "\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: \"\\n\".join(doc.page_content for doc in x[\"docs\"]),\n",
    "    \"question\": RunnablePassthrough()\n",
    "}\n",
    "\n",
    "answer = {\n",
    "    \"answer\": final_inputs | ANSWER_PROMPT | ChatOpenAI() | StrOutputParser(),\n",
    "    \"docs\": RunnablePassthrough()\n",
    "}\n",
    "\n",
    "final_chain = rephrase_chain | retrieved_documents | answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0a221f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'Yes, the dog loves to eat pizza.', 'docs': {'docs': [Document(metadata={'source': 'animal.txt'}, page_content='the dog loves to eat pizza'), Document(metadata={'source': 'animal.txt'}, page_content='the dog loves to eat pizza'), Document(metadata={'source': 'animal.txt'}, page_content='the cat loves to eat lasagna'), Document(metadata={'source': 'animal.txt'}, page_content='the cat loves to eat lasagna')], 'question': 'Is that really what the dog likes to eat?'}}\n"
     ]
    }
   ],
   "source": [
    "result = final_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"No, really?\",\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"What does the dog like to eat?\"),\n",
    "            AIMessage(content=\"Thuna!\")\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3224a538",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß© Dettaglio del Flusso\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "A[chat_history + question] --> B[Rephrase Chain]\n",
    "B --> C[Rephrased Question]\n",
    "C --> D[Retriever]\n",
    "C --> E[Prompt RAG]\n",
    "D --> F[Context]\n",
    "F --> E\n",
    "E --> G[ChatOpenAI]\n",
    "G --> H[StrOutputParser]\n",
    "H --> I[Final Answer]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Vantaggi del Rewriting\n",
    "\n",
    "| Problema                | Soluzione offerta                        |\n",
    "| ----------------------- | ---------------------------------------- |\n",
    "| Domande ambigue         | Rewriting in domande auto-contenute      |\n",
    "| Retrieval inefficace    | Migliore similarit√† semantica            |\n",
    "| Hallucinations dell‚ÄôLLM | Prompt RAG basato solo su contesto       |\n",
    "| Debug e tracciamento    | Output strutturato: risposta + documenti |\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Conclusione\n",
    "\n",
    "* Usare la **cronologia delle chat** migliora drasticamente il contesto.\n",
    "* Rewriting + Retrieval + RAG = pipeline completa e robusta.\n",
    "* LangChain e LCEL permettono di gestire questa logica in modo modulare ed elegante.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Suggerimento per la pratica\n",
    "\n",
    "> Modifica i documenti, cambia la domanda, analizza i documenti recuperati e la risposta generata.\n",
    "> üîÑ Pi√π ci giochi, meglio comprenderai i meccanismi di LCEL!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
