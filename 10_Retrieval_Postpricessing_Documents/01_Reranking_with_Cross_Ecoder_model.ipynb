{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0658d42",
   "metadata": {},
   "source": [
    "\n",
    "# üß† Post-Processing dei Documenti nella Pipeline RAG\n",
    "\n",
    "## üéØ Obiettivo\n",
    "\n",
    "Ridurre la quantit√† di documenti **passati al LLM** dopo il retrieval, mantenendo **solo i pi√π rilevanti** per la domanda dell‚Äôutente.\n",
    "Due strategie principali:\n",
    "\n",
    "1. **Reranking con Cross-Encoder**\n",
    "2. **Compressione con LLM** *(nella prossima lezione)*\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Strategia 1: Reranking con Cross-Encoder\n",
    "\n",
    "### ‚ú≥Ô∏è Cos‚Äô√® un Cross-Encoder?\n",
    "\n",
    "Un **Cross-Encoder** prende in input una **coppia (query, documento)** e calcola un **singolo punteggio** di similarit√†, elaborando entrambi insieme.\n",
    "‚úÖ **Alta accuratezza**\n",
    "‚ùå **Bassa efficienza** (non produce embedding riutilizzabili)\n",
    "\n",
    "> üîÑ Diverso dal Bi-Encoder, che calcola separatamente gli embedding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839df93b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üì¶ Setup iniziale (pipeline RAG classica)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e259de53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "loader = DirectoryLoader(\"./data\", glob=\"**/*.txt\")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=120,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    ")\n",
    "\n",
    "embedding_function = OpenAIEmbeddings()\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "db = Chroma.from_documents(chunks, embedding_function)\n",
    "\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d4ad1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs: list[Document]):\n",
    "    return \"\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c352874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question besed only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creiamo la RAG pipeline\n",
    "template = \"\"\"Answer the question besed only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02e3690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b12ee207",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rag_chain_with_source.invoke(input=\"Who is the owner of the restaurant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c5a1402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(metadata={'source': 'data\\\\founder.txt'}, page_content='Creating Chef Amico‚Äôs Restaurant'),\n",
       "  Document(metadata={'source': 'data\\\\restaurant.txt'}, page_content=\"into Chef Amico. Her mission was to uncover the secret behind the restaurant's growing fame. She was greeted by Amico\"),\n",
       "  Document(metadata={'source': 'data\\\\restaurant.txt'}, page_content='One evening, as the sun cast a golden glow over the city, a renowned food critic, Elena Rossi, stepped into Chef Amico.'),\n",
       "  Document(metadata={'source': 'data\\\\founder.txt'}, page_content='and relish life‚Äôs simple pleasures. His restaurant was a haven where strangers became friends over plates of arancini')],\n",
       " 'question': 'Who is the owner of the restaurant',\n",
       " 'answer': 'Chef Amico is the owner of the restaurant.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb3eb1a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üö´ Perch√® tale approccio √® negativo? \n",
    "\n",
    "Problema: recupero statico Top-K\n",
    "\n",
    "> ‚ùå Recupera sempre i documenti Top-K, **anche se poco rilevanti**. Non sappiamo quanto siano adatti a rispondere effettivamente alla domanda.\n",
    "\n",
    "\n",
    "## Cross Encoder Model\n",
    "\n",
    "- Per affrontare tale problema, possiamo utilizzare un `Cross Encoder`\n",
    "\n",
    "- Un `Cross Encoder` √® un tipo di modello che prende una coppia di input, come una query e un document, e li elabora assieme per predire un singolo punteggio che indica la rilevanza o la somiglianza\n",
    "\n",
    "> Perch√® non usare un modello di embedding?\n",
    "\n",
    "- Un modello di embedding √® un cosidetto modello di codifica (bi-Encoder)\n",
    "- Un Bi-Ecoder model genera embedding di frasi indipendenti, consentendo confronti efficaci tra grandi insiemi di dati.\n",
    "\n",
    "- Quindi √® molto utile per tasks come il recupero di informazioni, la ricerca semantica o il clustering \n",
    "\n",
    "- Con il **Cross-Encoder** model, d'altra parte, elabora le coppie di frasi insieme e predice direttamente un punteggio di somiglianza che ofre un'accuratteza molto pi√π elevata, ma manca dell'efficienza nella versatilit√† di generare embeddings riutilizzabili.\n",
    "\n",
    "- La raccomandazione √® di utilizzare i Cross-Encoders ogni volta che si dispone di un insieme predefinito di coppie di frasi a cui si vuole attribuire un punteggio, come 20 documenti e una query.\n",
    "\n",
    "- I Cross-Encoder sono pi√π lenti, ma raggiungono prestazioni migliori rispetto ai Bi-Encoders \n",
    "\n",
    "![alt](../images/cross_encoder.png)\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Reranking con `sentence-transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d63cb388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d6d0cc2f504b86bc328c2de050a6d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0062128e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2535cc9c6cc47e88b90feb544307276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\felip\\Desktop\\import-pc\\Advanced_RAG\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\felip\\.cache\\huggingface\\hub\\models--cross-encoder--ms-marco-MiniLM-L-6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9165813c9964e54b3ad3f0f84915881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c52257a61b449fc95cda9e66b6b44c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc77868555774e99ac44c01ca016c74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89db4d77b65493d938482396ba18a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58669ab139c742a19f50f3f8d5665db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b7b2b5fcab407eac3beea6f561f726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14317b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ora creiamo le coppie di frasi \n",
    "docs = result['context'] # lista dei Ducument\n",
    "\n",
    "contents = [doc.page_content for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd0802cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Creating Chef Amico‚Äôs Restaurant',\n",
       " \"into Chef Amico. Her mission was to uncover the secret behind the restaurant's growing fame. She was greeted by Amico\",\n",
       " 'One evening, as the sun cast a golden glow over the city, a renowned food critic, Elena Rossi, stepped into Chef Amico.',\n",
       " 'and relish life‚Äôs simple pleasures. His restaurant was a haven where strangers became friends over plates of arancini']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e75c5f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Who is the owner of the  restaurant', 'Creating Chef Amico‚Äôs Restaurant'],\n",
       " ['Who is the owner of the  restaurant',\n",
       "  \"into Chef Amico. Her mission was to uncover the secret behind the restaurant's growing fame. She was greeted by Amico\"],\n",
       " ['Who is the owner of the  restaurant',\n",
       "  'One evening, as the sun cast a golden glow over the city, a renowned food critic, Elena Rossi, stepped into Chef Amico.'],\n",
       " ['Who is the owner of the  restaurant',\n",
       "  'and relish life‚Äôs simple pleasures. His restaurant was a haven where strangers became friends over plates of arancini']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creiamo le coppie\n",
    "pairs = []\n",
    "\n",
    "for text in contents:\n",
    "    pairs.append([\"Who is the owner of the  restaurant\", text])\n",
    "\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3615c98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.4635894, -3.478981 , -4.9786096, -5.300245 ], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_encoder.predict(pairs)\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5220dd6",
   "metadata": {},
   "source": [
    "I valori assoluti del non hanno importanza. Quello importante √® l'ordine dei numeri.\n",
    "\n",
    "Quindi dobbiamo in qualche modo riunire tutto questo con i documenti. \n",
    "\n",
    "Per questo possiamo usare la funzione zip e poi usare la funzione sorted integrata per ordinare i documenti in ordine crescente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a3c1dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(np.float32(-3.4635894),\n",
       "  Document(metadata={'source': 'data\\\\founder.txt'}, page_content='Creating Chef Amico‚Äôs Restaurant')),\n",
       " (np.float32(-3.478981),\n",
       "  Document(metadata={'source': 'data\\\\restaurant.txt'}, page_content=\"into Chef Amico. Her mission was to uncover the secret behind the restaurant's growing fame. She was greeted by Amico\")),\n",
       " (np.float32(-4.9786096),\n",
       "  Document(metadata={'source': 'data\\\\restaurant.txt'}, page_content='One evening, as the sun cast a golden glow over the city, a renowned food critic, Elena Rossi, stepped into Chef Amico.')),\n",
       " (np.float32(-5.300245),\n",
       "  Document(metadata={'source': 'data\\\\founder.txt'}, page_content='and relish life‚Äôs simple pleasures. His restaurant was a haven where strangers became friends over plates of arancini'))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scored_docs = zip(scores, docs)\n",
    "\n",
    "sorted_docs = sorted(scored_docs, reverse=True)\n",
    "\n",
    "sorted_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a60757e",
   "metadata": {},
   "source": [
    "Come si pu√≤ vedere il document con \"Creating Chef Amic's Restaurant'\" √® quello con il punteggio pi√π alto per rispondere alla domanda \"Who is the owner of the restuarant\".\n",
    "\n",
    "Ora ci ritroviamo con un elenco di punteggi e documenti e possiamo logicamnete rimuovere il punteggio e utilizzare un indice per ridurre il numero di documenti da passare a un LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "924edfa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data\\\\founder.txt'}, page_content='Creating Chef Amico‚Äôs Restaurant'),\n",
       " Document(metadata={'source': 'data\\\\restaurant.txt'}, page_content=\"into Chef Amico. Her mission was to uncover the secret behind the restaurant's growing fame. She was greeted by Amico\")]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranked_docs = [doc for _, doc in sorted_docs][0:2]\n",
    "\n",
    "reranked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0539f73",
   "metadata": {},
   "source": [
    "Questo √® un passo molto importanate, perch√® non si ordina solo per riordinare i documenti, ma per ridurre il numero di documenti da inviare a un LLM. \n",
    "\n",
    "Supponiamo di aver recuperato 20 documenti ma di voler inviare solo i quattro pi√π importanti. \n",
    "\n",
    "Ora vogliamo integrare questi passaggi fatti a mano in una pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7599c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever il quale recupera pi√π di 4 documents\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7999252",
   "metadata": {},
   "source": [
    "creiamo i passaggi precedenti in un'unica funzione in cui:\n",
    "\n",
    "- passiamo i dati di input, \n",
    "- estraiamo anche i documenti \n",
    "- creiamo le nostre coppie \n",
    "- usiamo il modello CrossEncoder\n",
    "- riordiniamo i documenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51bbce76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def rerank_documents(input_data):\n",
    "    query = input_data['question']\n",
    "    docs = input_data['context']\n",
    "\n",
    "    cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "    contents = [doc.page_content for doc in docs]\n",
    "\n",
    "    pairs = [(query, text) for text in contents]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "\n",
    "    scored_docs = zip(scores, docs)\n",
    "\n",
    "    sorted_docs = sorted(scored_docs, key=lambda x: x[0], reverse=True) # ordina in base al punteggio\n",
    "\n",
    "    return [doc for _, doc in sorted_docs]\n",
    "\n",
    "# integriamo tale funzione in LCEL usando RunnableLambda\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question_ {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=RunnableLambda(rerank_documents))\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()} # questi due vengono eseguiti in parallelo (contemporaneamente) su uno stesso input (question)\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fbfe248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001A2A5DBBDF0>, search_kwargs={'k': 10}),\n",
       "  question: RunnablePassthrough()\n",
       "}\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableAssign(mapper={\n",
       "              context: RunnableLambda(lambda x: x[0])\n",
       "            })\n",
       "            | ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion_ {question}\\n'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001A30F502E30>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001A30F502C80>, root_client=<openai.OpenAI object at 0x000001A30F5031F0>, root_async_client=<openai.AsyncOpenAI object at 0x000001A30F502200>, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser()\n",
       "  })"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain_with_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "329f9d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(metadata={'source': 'data\\\\founder.txt'}, page_content='Creating Chef Amico‚Äôs Restaurant'),\n",
       "  Document(metadata={'source': 'data\\\\restaurant.txt'}, page_content=\"into Chef Amico. Her mission was to uncover the secret behind the restaurant's growing fame. She was greeted by Amico\"),\n",
       "  Document(metadata={'source': 'data\\\\restaurant.txt'}, page_content='One evening, as the sun cast a golden glow over the city, a renowned food critic, Elena Rossi, stepped into Chef Amico.'),\n",
       "  Document(metadata={'source': 'data\\\\founder.txt'}, page_content='and relish life‚Äôs simple pleasures. His restaurant was a haven where strangers became friends over plates of arancini'),\n",
       "  Document(metadata={'source': 'data\\\\founder.txt'}, page_content='Philosophy of Hospitality'),\n",
       "  Document(metadata={'source': 'data\\\\founder.txt'}, page_content='young chefs, shares his knowledge at culinary workshops, and supports local farmers and producers.'),\n",
       "  Document(metadata={'source': 'data\\\\founder.txt'}, page_content='culmination of his travels and a tribute to his Sicilian roots. Nestled in a quaint corner of the city, the restaurant'),\n",
       "  Document(metadata={'source': 'data\\\\founder.txt'}, page_content='the restaurant quickly gained fame for its authentic flavors and Amico‚Äôs innovative twists on traditional recipes.'),\n",
       "  Document(metadata={'source': 'data\\\\founder.txt'}, page_content='The Journey to Chef Amico'),\n",
       "  Document(metadata={'source': 'data\\\\founder.txt'}, page_content='craft. His spirit of generosity and passion for food extends beyond the restaurant‚Äôs walls. He mentors young chefs,')],\n",
       " 'question': 'Who is the owner of the rastaurant',\n",
       " 'answer': 'Chef Amico is the owner of the restaurant.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = rag_chain_with_source.invoke(input=\"Who is the owner of the rastaurant\")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820882d9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìà Vantaggi del Reranking\n",
    "\n",
    "| ‚úÖ Vantaggi                            | ‚ùå Limiti                         |\n",
    "| ------------------------------------- | -------------------------------- |\n",
    "| Alta precisione                       | Pi√π lento dei Bi-Encoder         |\n",
    "| Utile quando i documenti sono ambigui | Valuta solo **relativamente**    |\n",
    "| Riduce numero di documenti al LLM     | Non filtra assolutamente l‚Äôutile |\n",
    "\n",
    "\n",
    "![alt](../images/drawbacks_cross_encoder.png)\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Conclusione\n",
    "\n",
    "Il **Cross-Encoder** migliora la qualit√† dei documenti recuperati e riduce il carico sul LLM.\n",
    "Tuttavia, non **filtra** completamente i documenti irrilevanti.\n",
    "‚û°Ô∏è Per questo, serve la **compressione basata su LLM**, che vedremo nella prossima lezione.\n",
    "\n",
    "---\n",
    "\n",
    "üéì **Prossima lezione**: `LLM-based document compression` per filtrare in modo \"intelligente\" documenti poco informativi usando la comprensione semantica di un LLM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
